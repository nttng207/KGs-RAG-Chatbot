# -*- coding: utf-8 -*-
"""Get_Index.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OFmSDN9FXoEAJP2hiQqMn7HSegDocvJ3
"""

# !pip install llama_index
# !pip install transformers accelerate bitsandbytes
# !pip install pypdf
# !pip install neo4j
#!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python

from llama_index import (
    SimpleDirectoryReader,
    VectorStoreIndex,
    ServiceContext,
    download_loader,
    PromptHelper,
    StorageContext,
    load_index_from_storage,
    StorageContext,
    KnowledgeGraphIndex,
)
from llama_index.query_engine import KnowledgeGraphQueryEngine
from llama_index.graph_stores import Neo4jGraphStore
from llama_index.embeddings import HuggingFaceEmbedding, OpenAIEmbedding, LangchainEmbedding
from llama_index.response.notebook_utils import display_response
from pathlib import Path
from llama_index import download_loader
from llama_index.llms import LlamaCPP
from llama_index.text_splitter import SentenceSplitter


#---------------------------------------#
import torch
from transformers import BitsAndBytesConfig
from llama_index.prompts import PromptTemplate
from llama_index.callbacks import CallbackManager, LlamaDebugHandler
from llama_index.llms import HuggingFaceLLM
from llama_index.llms.llama_utils import (
    messages_to_prompt,
    completion_to_prompt,
)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)
import logging
import sys
import os

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

llama_debug = LlamaDebugHandler(print_trace_on_end=True)
callback_manager = CallbackManager([llama_debug])
model_url = "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf?download=true"

# Change LLMs

 llm = LlamaCPP("......")

#------------------------Read file------------------#


PDFReader = download_loader("PDFReader")

loader = PDFReader()
documents = loader.load_data(file=Path('......'))

#--------------------------Define service_context----------------------------#

from llama_index.node_parser import SentenceSplitter
#embed_model = OpenAIEmbedding() #sentence-transformers/all-roberta-large-v1
embed_model = LangchainEmbedding(HuggingFaceEmbedding("sentence-transformers/all-roberta-large-v1")) #local:BAAI/bge-small-en-v1.5
service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, chunk_size = 128, chunk_overlap = 120)

#---------------Set up database------------------#

username = "neo4j"
password = "7q0G7tP_ZxJGK-HH87pNoLMqcCO4sDN2cQnPIASwzLw"
url = "neo4j+s://39992655.databases.neo4j.io"
database = "neo4j"

graph_store = Neo4jGraphStore(
    username=username,
    password=password,
    url=url,
    database=database,
)

storage_context = StorageContext.from_defaults(graph_store=graph_store)

#--------------------Custom data for better documents indexing and understanding------------------#

splitter = SentenceSplitter(
    chunk_size=1024,
    chunk_overlap=20,
)
nodes = splitter.get_nodes_from_documents(documents)

#------------------Get index---------------------#

index = KnowledgeGraphIndex.from_documents(
    doc,
    storage_context=storage_context,
    max_triplets_per_chunk=2, #10, 15, 20
    #kg_triplet_extract_fn=extract_triplets,
    service_context=service_context,
    #kg_triple_extract_template=prompt_template,
    #include_embedding = True,
    show_progress = True
)

index.storage_context.persist(persist_dir = './content/graph_chatbot') # Save the index for future retrieval